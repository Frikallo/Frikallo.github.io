<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Home on AetherAI</title><link>https://aetherai.xyz/</link><description>Recent content in Home on AetherAI</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 02 Feb 2022 11:00:00 -0500</lastBuildDate><atom:link href="https://aetherai.xyz/index.xml" rel="self" type="application/rss+xml"/><item><title>About Us</title><link>https://aetherai.xyz/about/</link><pubDate>Wed, 02 Feb 2022 11:00:00 -0500</pubDate><guid>https://aetherai.xyz/about/</guid><description>EleutherAI (/iˈluθər eɪ. aɪ/) is a decentralized collective of volunteer researchers, engineers, and developers focused on AI alignment, scaling, and open source AI research. Founded in July of 2020, we are most well known for our ongoing efforts to build and open source large language models, but we also do open research in alignment, interpretability, BioML, ML art and many other fields. Our Discord server is open and welcomes contributors!</description></item><item><title>Frequently Asked Questions</title><link>https://aetherai.xyz/faq/</link><pubDate>Wed, 02 Feb 2022 11:00:00 -0500</pubDate><guid>https://aetherai.xyz/faq/</guid><description>The official EleutherAI FAQ, covering every question you probably have about EleutherAI.</description></item><item><title>GPT-NeoX</title><link/><pubDate>Wed, 02 Feb 2022 11:00:00 -0500</pubDate><guid/><description>Announcing GPT-NeoX-20B, a 20 billion parameter model trained in collaboration with CoreWeave.</description></item><item><title>Publications</title><link>https://aetherai.xyz/publications/</link><pubDate>Wed, 02 Feb 2022 11:00:00 -0500</pubDate><guid>https://aetherai.xyz/publications/</guid><description>A list of EleutherAI-affiliated publications.</description></item><item><title>Get Involved</title><link>https://aetherai.xyz/get-involved/</link><pubDate>Fri, 18 Jun 2021 00:00:00 -0400</pubDate><guid>https://aetherai.xyz/get-involved/</guid><description>Joining EleutherAI is as simple as joining us on Discord and picking a project to contribute to.
As an independent organization, we are dependent upon donations to power our compute. If you have a very large quantity of compute or compute credits, please contact us directly at contact[at]eleuther.ai
Contributors are traditionally expected to have at some experience with machine learning and data science. While there is some work for people without that skillset, we are a DL research lab and therefore most work involves writing, training, and inferencing neural networks with industry-standard frameworks.</description></item><item><title>GPT-J</title><link/><pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate><guid/><description>GPT-J-6B, a 6 billion parameter model trained on the Pile, is now available for use with our new codebase, Mesh Transformer JAX.</description></item><item><title>Blog</title><link/><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid/><description>We believe the creation and open source release of a large language model is a net good to AI safety. We explain why.</description></item><item><title>Blog</title><link/><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid/><description>Rotary Positional Embedding (RoPE) is a new type of position encoding that unifies absolute and relative approaches. We put it to the test.</description></item><item><title>GPT-Neo</title><link/><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid/><description>GPT-Neo 1.3B and 2.7B are now available on Hugging Face Model Hub! Run the models with Transformers or call for them through their on-demand Inference API.</description></item><item><title>GPT-Neo</title><link/><pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate><guid/><description>GPT-Neo 1.3B and 2.7B, trained on the Pile, are now available to run with the GPT-Neo framework.</description></item><item><title>The Pile</title><link/><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid/><description>We are proud to announce the release of the Pile, a free and publicly available 825GB dataset of diverse English text for language modeling!</description></item><item><title>AlphaFold2 Replication</title><link>https://aetherai.xyz/projects/alphafold2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/alphafold2/</guid><description>A replication of AlphaFold2 architecture (very accurate protein structure prediction model) with a free licensing.</description></item><item><title>CARP</title><link>https://aetherai.xyz/projects/carp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/carp/</guid><description>Contrastive learning for story critiques.</description></item><item><title>CLASP</title><link>https://aetherai.xyz/projects/clasp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/clasp/</guid><description>A CLIP-like model for amino acid sequence prediction.</description></item><item><title>Eval Harness</title><link>https://aetherai.xyz/projects/lm-eval/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/lm-eval/</guid><description>Github Repo: https://github.com/EleutherAI/lm-evaluation-harness</description></item><item><title>GPT-Neo</title><link>https://aetherai.xyz/projects/gpt-neo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/gpt-neo/</guid><description>An implementation of model &amp;amp; data-parallel autoregressive language models with Mesh Tensorflow for distributed TPUs.</description></item><item><title>GPT-NeoX</title><link>https://aetherai.xyz/projects/gpt-neox/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/gpt-neox/</guid><description>An implementation of 3D-parallel autoregressive language models for distributed GPUs.</description></item><item><title>Mesh Transformer JAX</title><link>https://aetherai.xyz/projects/mesh-transformer-jax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/mesh-transformer-jax/</guid><description>An implementation of model &amp;amp; data-parallel autoregressive language models with JAX and Haiku for distributed TPUs.</description></item><item><title>OpenWebText2</title><link>https://aetherai.xyz/projects/owt2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/owt2/</guid><description>An enhanced version of OpenWebTextCorpus.</description></item><item><title>The Pile</title><link>https://aetherai.xyz/projects/pile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aetherai.xyz/projects/pile/</guid><description>A large, diverse, open-source language modelling dataset.</description></item></channel></rss>