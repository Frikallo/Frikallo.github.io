<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=robots content="index, follow"><title>Frequently Asked Questions | AetherAI</title><meta name=description content="The official EleutherAI FAQ, covering every question you probably have about EleutherAI."><link rel=canonical href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/faq/><link crossorigin=anonymous href=/Frikallo/Frikallo.github.io/tree/gh-pages/assets/css/stylesheet.min.538a5eddf95a593a32b416b321b420df71201416cb0038c0beea7528d0de0113.css integrity="sha256-U4pe3flaWToytBazIbQg33EgFBbLADjAvup1KNDeARM=" rel="preload stylesheet" as=style><link rel=preload href=/images/libre.svg as=image><link rel=preload href=/fonts/eleuther.woff2 as=font type=font/woff2 crossorigin><link rel=icon href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/favicon/apple-touch-icon.png><link rel=mask-icon href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/safari-pinned-tab.svg><link rel=manifest href=/manifest.webmanifest><meta name=apple-mobile-web-app-capable content="yes"><meta name=theme-color content="#000"><meta name=application-name content><meta name=msapplication-TileColor content="#000"><meta name=color-scheme content="light dark"><meta name=generator content="Hugo 0.95.0"><script async src="https://www.googletagmanager.com/gtag/js?id=G-4SDKJXT0HF"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-4SDKJXT0HF",{anonymize_ip:!1})}</script><meta property="og:title" content="Frequently Asked Questions"><meta property="og:description" content="The official EleutherAI FAQ, covering every question you probably have about EleutherAI."><meta property="og:type" content="website"><meta property="og:url" content="https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/faq/"><meta property="og:image" content="https://eleuther.ai/images/promo.png"><meta property="og:site_name" content="AetherAI"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://eleuther.ai/images/promo.png"><meta name=twitter:title content="Frequently Asked Questions"><meta name=twitter:description content="The official EleutherAI FAQ, covering every question you probably have about EleutherAI."><script type=application/ld+json>{"@context":"https://schema.org","@id":"https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/faq/","@type":"FAQPage","name":"Frequently Asked Questions","publisher":{"@id":"https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/#org","@type":"Organization","name":"AetherAI","url":"https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/"}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><div class=nav-container><a class=logo href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/ accesskey=h title="AetherAI (Alt + H)"><img src=/images/libre.svg alt=logo aria-label=logo width=48 height=48><span class=logotype>AetherAI</span></a><nav class=nav><ul id=menu><li><a href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/ title=Home><span>Home</span></a></li><li><a href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/about/ title=About><span>About</span></a></li><li><a href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/faq/ title=FAQ><span class=active>FAQ</span></a></li><li><a href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/publications/ title=Publications><span>Publications</span></a></li></ul><button id=theme-toggle accesskey=t title="Theme Toggle (Alt + T)"><img src=/images/halfCircle.svg alt="dark mode switch button"></button></nav></div></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Frequently Asked Questions</h1><div class=post-description>The official EleutherAI FAQ, covering every question you probably have about EleutherAI.</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#general aria-label=General>General</a></li><li><a href=#large-language-models aria-label="Large Language Models">Large Language Models</a></li></ul></div></details></div><div class=post-content><h2 id=general>General<a hidden class=anchor aria-hidden=true href=#general>#</a></h2><dl class=qa><dt class=question>How did this all start?</dt><dd class=answer><p>One day, <a href=https://github.com/ConnorJL>Connor Leahy</a> posted in the TPU Podcast Discord:</p><figure><article class=discord><div class="chat-message groupStart"><div class=contents><img loading=lazy alt class=avatar src=https://cdn.discordapp.com/avatars/157923989262434304/ea2d9c1e1a063d890a948c48ef962c8d.png width=40 height=40><div class=header><span class=headerText><span class=role-regular>Daj</span></span><span class=timestamp><time datetime=2020-07-02T07:50:55.844Z>2020-07-02</time></span></div><div class=messageContent><a href=https://arxiv.org/abs/2006.16668>https://arxiv.org/abs/2006.16668</a><br>Hey guys lets give OpenAI a run for their money like the good ol&rsquo; days</div></div></div></article></figure><p>To which <a href=https://github.com/leogao2>Leo Gao</a> replied:<figure><article class=discord><div class="chat-message groupStart"><div class=contents><img loading=lazy alt class=avatar src=https://web.archive.org/web/20211004054112im_/https://cdn.discordapp.com/avatars/606987544235868219/89c76c2459fb48c8856ecff4f33d4fd7.png width=40 height=40><div class=header><span class=headerText><span class=role-regular>bmk</span></span><span class=timestamp><time datetime=2020-07-02T16:03:05.471Z>2020-07-02</time></span></div><div class=messageContent><span class=mention>@Daj</span> this but unironically</div></div></div></article></figure></p><p>And so it began.</p></dd><dt class=question>Where did the name come from?</dt><dd class=answer><p>In Ancient Greek, <a href=https://en.wikipedia.org/wiki/Eleutheria><em>eleutheria</em></a> is a word for &ldquo;liberty&rdquo;, and was used as a proper noun as a personification of the concept. This same personage became <a href=https://en.wikipedia.org/wiki/Libertas><em>Libertas</em></a> to the Romans and <a href=https://en.wikipedia.org/wiki/Statue_of_Liberty><em>Lady Liberty</em></a> to Americans.</p></dd><dt class=question>So . . . what&rsquo;s the deal with your logo?</dt><dd class=answer><p>Keeping with the theme, our logotype and all images (other than the logo) on this website were generated with deep learning techniques. Our triangular logo was designed by Sid Black and later refined by Eric Hallahan.</p></dd><dt class=question>What ways are there to support EleutherAI?</dt><dd class=answer><p>We are lucky to currently be flush with most material support we could need, and we are primarily bottlenecked by man-hours. What we need is energetic contributors that can <a href=https://board.eleuther.ai/>contribute to and lead projects</a> of their own! We are happy to support promising AI research, in particular alignment-relevant work.</p></dd><dt class=question>How can I get involved?</dt><dd class=answer><p>Join our <a href=https://discord.gg/zBGx3azzUn>Discord</a> or check us out on <a href=https://github.com/EleutherAI>GitHub</a>! We&rsquo;re an open community, so you are free to contribute as you wish. However, we expect newcomers either to be fairly knowledgeable or to sit on the sidelines until they understand the internal structure and culture of our operations.</p></dd><dt class=question>Where can I go if I have more questions?</dt><dd class=answer><p>For general questions, <a href=https://discord.gg/zBGx3azzUn>Discord</a> is the best place for that. Our founding members appear in <span class=role-O5>purple</span> and our core contributors appear in <span class=role-mathemagician>sea green</span> and <span class=role-level5>blue</span>. They will be able to provide helpful guidance or answer questions.</p><p>For more professional inquiry, drop us a line at <a href=mailto:contact@eleuther.ai>contact@eleuther.ai</a>.</p><p>We politely ask that you do not expect us to be your tech support; those who contribute to EleutherAI do so in their free time and tend to prefer contributing to projects rather than debugging your problems. We recommend consulting the corresponding documentation before asking us for help. If you think you have found a bug, please consider opening an issue on <a href=https://github.com/EleutherAI>GitHub</a>.</p></dd><dt class=question>I&rsquo;m new to deep learning&mdash;How do I get into AI? What is a transformer? Tell me how everything works!</dt><dd class=answer><p>We are a research-focused Discord server and not an educational one. We welcome beginners to lurk and talk about topics they are knowledgeable of, but this is not the place to get intro-level resources or answers to basic questions. We have links to several excellent beginner-friendly servers on <a href=https://discord.gg/zBGx3azzUn>Discord</a> in the <span class=mention>#communities</span> channel.</p></dd></dl><h2 id=large-language-models>Large Language Models<a hidden class=anchor aria-hidden=true href=#large-language-models>#</a></h2><dl class=qa><dt class=question>What are GPT-Neo and GPT-NeoX?</dt><dd class=answer><p><a href=https://github.com/EleutherAI/gpt-neo>GPT-Neo</a> and <a href=https://github.com/EleutherAI/gpt-neox>GPT-NeoX</a> are our codebases for training massive language models, for which are released under open licenses. The models themselves are referred to by their size (in millions or billions of parameters).</p><p>All active work is in GPT-NeoX, and GPT-Neo should be considered deprecated. For those looking for code that runs on TPUs, we recommend <a href=https://github.com/kingoflolz/mesh-transformer-jax>Mesh Transformer JAX</a> instead.</p></dd><dt class=question>How big is the largest model you have trained?</dt><dd class=answer><p>On <date datetime=2022-02-09>February 9, 2022</date>, we will release a 20 billion parameter model trained on the Pile, GPT-NeoX-20B.</p></dd><dt class=question>Are you serious when you say you are going to train a model comparable to the biggest GPT-3 (175 billion parameters)?</dt><dd class=answer><p>Yes, that is the plan. We expect our final model to be somewhere between 150 and 200 billion parameters.</p></dd><dt class=question>Have you considered the possible risks of creating models like these?</dt><dd class=answer><p><a href=https://blog.eleuther.ai/why-release-a-large-language-model/>Yes, we have considered the risks of creating and releasing such models at length, and have come to the conclusion that we believe the benefits outweigh the risks.</a></p></dd><dt class=question>When do you plan to have more models available?</dt><dd class=answer><p>As a collective of volunteer researchers and engineers who contribute in our free time, we are unable to commit to either a timeline or a roadmap for future models.</p></dd><dt class=question>How are you training such large models?</dt><dd class=answer><p>For GPT-Neo and GPT-J, we utilized our access to preemptible TPUs through the <a href=https://sites.research.google/trc/>TPU Research Cloud (TRC)</a>.</p><p>For GPT-NeoX, we have been graciously offered high-performance GPU compute by <a href=https://www.coreweave.com/>CoreWeave</a>. CoreWeave is excited by the open nature of the project and has been instrumental in helping us scale our efforts to train larger autoregressive language models.</p></dd><dt class=question>What differentiates GPT-Neo, GPT-NeoX, and Mesh Transformer JAX?<br>Why develop so many codebases?</dt><dd class=answer><p>Built from the ground up upon <a href=https://github.com/tensorflow/mesh>Mesh Tensorflow</a> for training on TFRC TPUs, GPT-Neo was our first attempt at building a codebase that could scale to many billions of parameters. The Hobson&rsquo;s choice of Mesh Tensorflow did not come without consequences; the TPU-first design of the framework forever constrained GPT-Neo. This, in addition to <a href=https://blog.eleuther.ai/year-one/#the-tensorflow-days>the desire for a codebase less prone to having contributors want to pull their hair out</a>, led us to pursue alternative means for our further endeavors in scaling. Our distaste with the codebase did not stop us from putting it to good use though&mdash;it most notably resulted in GPT-Neo 1.3B and 2.7B, released <date datetime=2021-03-21>March 21, 2021</date>.</p><p>Apart from appending the 24th letter of the ISO basic Latin alphabet, GPT-NeoX is our GPU codebase. Built upon <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> and <a href=https://www.deepspeed.ai/>DeepSpeed</a>, it was motivated by our access to compute resources: It is unrealistic for us to train models larger than a few tens of billions of parameters on TRC TPUs in reasonable time due to the pre-empting of instances. CoreWeave offered us a path to train models at the scales we wanted, but we needed to utilize their GPUs for training instead of TPUs. As such, it made sense to build a new codebase to take full advantage of their GPU hardware.</p><p>Mesh Transformer JAX came as a stopgap solution to the woes of an <a href=https://en.wikipedia.org/wiki/2020%E2%80%93present_global_chip_shortage>international chip shortage</a> blocking much of GPT-NeoX development throughout the spring and summer of <date datetime=2021>2021</date>. What started as work towards a <a href=https://openai.com/blog/dall-e/>DALL-E</a>-like multimodal model was soon rescoped to train more traditional language models. Built upon <a href=https://github.com/google/jax>JAX</a> and <a href=https://github.com/deepmind/dm-haiku>Haiku</a> with the <a href=https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms>then-new TPU VM</a> in mind, it is the designated successor to the original GPT-Neo codebase for training models on TPUs. This project ultimately resulted in the release of GPT-J-6B on <date datetime=2021-06-08>June 8, 2021</date>.</p></dd><dt class=question>What about volunteer-driven distributed computing, like <a href=https://boinc.berkeley.edu/>BOINC</a>, <a href=https://foldingathome.org/>Folding@Home</a>, or <a href=https://github.com/learning-at-home/hivemind>hivemind</a>?</dt><dd class=answer><p>We have considered the possibility of pooling volunteer resources for training models, but upon thorough review, we have concluded that such approaches are not a viable option today. There are numerous problems with current distributed approaches for us:</p><ul><li>Backpropagation is dense and sensitive to precision, therefore requiring high-bandwidth communication. Consumer-grade internet connections are wholly insufficient.</li><li>Mixture-of-experts-based models tend to significantly underperform monolithic (regular) models for the same number of parameters.</li><li>Having enough contributors to outweigh the high overhead is infeasible.</li><li>Verifiability and resistance to outside attack are not currently possible without significant additional overhead.</li></ul><p>In short, doing volunteer-driven distributed compute well for this use case is an unsolved problem.</p></dd><dt class=question>Have you considered more efficient architectures or methods?</dt><dd class=answer><p>Our intention is not to perfectly replicate the architecture used by GPT-3 but to instead build models comparable to what OpenAI has built. We are committed to exploring the entire space of architectures and methods, including various linear-scaling attention mechanisms, mixture-of-experts, and other designs. However, in our experience, these designs are not always well suited to language modeling: Attention mechanisms that scale with linear complexity with respect to sequence length are often strictly incompatible with the autoregressive objective used for text generation; the remaining methods have faired poorly in our testing. Engineering is full of trade-offs, and silver-bullet research breakthroughs are uncommon occurrences. <a href=https://blog.eleuther.ai/rotary-embeddings>If and when new methodologies surpass what we have already, we will integrate and use them.</a></p></dd><dt class=question>Will I be able to run models on my computer locally, offline?</dt><dd class=answer><p>The answer is highly dependent on hardware and configuration.</p><p>No, you will not be able to run a model the size of full-scale GPT-3 on your first-generation Macbook Air. 175 billion parameters at single-precision (binary32) take up 700 Gigabytes, and realistically the entire model needs to be loaded into memory for inference. It is unlikely that consumer hardware will be able to run anything of that scale for years to come, even on CPU. To run large models beyond a few billion parameters there is an expectation to utilize systems with large amounts of compute and memory.</p><p>Smaller models can be run on more pedestrian hardware: 125 million parameters take up only 500 Megabytes and should run on a basic laptop without a hitch, while 1.3 billion parameters take up 5 Gigabytes and should run on capable personal computers without issue.</p><p>If you are interested in inferencing and fine-tuning models, we can recommend using the implementations of <a href=https://huggingface.co/transformers/model_doc/gpt_neo.html>GPT-Neo</a> and <a href=https://huggingface.co/transformers/model_doc/gptj.html>GPT-J</a> found in Hugging Face Transformers, which is far easier to both install and use than our research code. We do not support or maintain the Hugging Face implementation beyond <a href=https://huggingface.co/eleutherai>our organization in Model Hub</a>, and issues with Transformers or its usage should be directed elsewhere (such as the <a href=https://discuss.huggingface.co/>Hugging Face community forums</a>).</p></dd><dt class=question>Are the codebases free software?</dt><dd class=answer><p>GPT-Neo is <a href=https://github.com/EleutherAI/gpt-neo/blob/master/LICENSE>MIT-licensed</a>, while GPT-NeoX is <a href=https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE>licensed under Apache 2.0</a>. These are the most freely-termed licenses that we can provide for each codebase respectively.</p><p>Mesh Transformer JAX is <a href=https://github.com/kingoflolz/mesh-transformer-jax/blob/master/LICENSE.txt>licenced under Apache 2.0</a>.</p></dd><dt class=question>Are the models free software?</dt><dd class=answer><p>EleutherAI is licensing models under Apache 2.0. If you use our models, we would highly appreciate you citing or displaying your usage of them.</p></dd><dt class=question>How should I cite your models?</dt><dd class=answer><p>We ask that you cite both the codebase and the dataset together when citing models. Our recommended citation method is as follows.</p><p><em>In the document body:</em><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-latex data-lang=latex><span style=display:flex><span>X.XB GPT-Neo <span style=color:#66d9ef>\citep</span>{gpt-neo} model trained on the Pile <span style=color:#66d9ef>\citep</span>{pile}</span></span></code></pre></div></p><p><em>BibTeX entries:</em><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bibtex data-lang=bibtex><span style=display:flex><span><span style=color:#a6e22e>@article</span>{pile,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>title</span>=<span style=color:#e6db74>{The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>author</span>=<span style=color:#e6db74>{Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>journal</span>=<span style=color:#e6db74>{arXiv preprint arXiv:2101.00027}</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>year</span>=<span style=color:#e6db74>{2020}</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## The Pile</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>What is the Pile? What is in it?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>: The Pile is a 825 GiB diverse, open source language modeling dataset that consists of 22 smaller, high-quality datasets combined together. For more information, please read the [paper](https://arxiv.org/abs/2101.00027) or the [datasheet](https://arxiv.org/abs/2201.07311) on arXiv.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>Who can use the Pile?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>: The Pile was primarily designed for researchers training large-scale language models. It also may be of interest to other researchers interested in topics such as bias, online discourse, and text compression.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>Where can I get the Pile?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>: [The data can be downloaded here](https://the-eye.eu/public/AI/pile/).</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>Can I add something to the Pile?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>: Pile v1 is finalized and is no longer accepting contributions.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>Have you considered adding Discord logs?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>: Yes. We decided against it, as there are good privacy reasons Discord users may not expect or want their conversations unwittingly added to a public dataset like this. Collecting such a dataset would most likely also violate [Discord&#39;s ToS](https://discord.com/terms). In general, more trouble than they&#39;re worth.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>Can I make my own version of the Pile?</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>: Of course! For just this reason, all of the components and the Pile creation process are reproducible. [The code used to create the Pile can be found here.](https://github.com/EleutherAI/the-pile) Links to the code for reproducing each component are also available at that repo.</span></span></span></code></pre></div></p></dd></dl></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://github.com/Frikallo/Frikallo.github.io/tree/gh-pages/>AetherAI</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>